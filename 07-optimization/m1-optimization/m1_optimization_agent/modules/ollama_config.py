#!/usr/bin/env python3
"""
Ollama Configuration Module
Handles Ollama environment variable configuration and optimization.
"""

import os
import subprocess
import json
from typing import Dict, Optional
from loguru import logger


class OllamaConfigurator:
    """Handles Ollama configuration for optimal M1 performance."""
    
    def __init__(self):
        self.config_file = "config/ollama_config.json"
        self.env_file = os.path.expanduser("~/.ollama_env")
        self._ensure_config_exists()
    
    def _ensure_config_exists(self):
        """Ensure configuration directory exists."""
        os.makedirs(os.path.dirname(self.config_file), exist_ok=True)
    
    def _run_command(self, command: list) -> Dict:
        """Execute a system command and return result."""
        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except Exception as e:
            logger.error(f"Command failed: {e}")
            return {"success": False, "error": str(e)}
    
    def configure(self, num_parallel: int = 1, max_loaded_models: int = 1,
                 keep_alive: str = "5m", context_size: Optional[int] = None) -> Dict:
        """Configure Ollama environment variables for optimal performance.
        
        Args:
            num_parallel: Number of parallel requests (1 for 8GB, 2 for 16GB)
            max_loaded_models: Maximum models to keep loaded in memory
            keep_alive: How long to keep models loaded (e.g., '5m', '10m', '-1' for infinite)
            context_size: Optional context window size (e.g., 2048, 4096)
        
        Returns:
            Dict with success status and applied configuration
        """
        logger.info(f"Configuring Ollama: parallel={num_parallel}, max_models={max_loaded_models}, keep_alive={keep_alive}")
        
        results = {
            "success": True,
            "configuration": {},
            "method": "launchctl"
        }
        
        try:
            # Define environment variables
            env_vars = {
                "OLLAMA_NUM_PARALLEL": str(num_parallel),
                "OLLAMA_MAX_LOADED_MODELS": str(max_loaded_models),
                "OLLAMA_KEEP_ALIVE": keep_alive
            }
            
            if context_size:
                env_vars["OLLAMA_NUM_CTX"] = str(context_size)
            
            # Method 1: Set via launchctl (persists for user session)
            for key, value in env_vars.items():
                result = self._run_command(['launchctl', 'setenv', key, value])
                if result["success"]:
                    results["configuration"][key] = value
                    logger.info(f"Set {key}={value} via launchctl")
                else:
                    logger.warning(f"Failed to set {key} via launchctl: {result.get('stderr', '')}")
            
            # Method 2: Write to shell environment file for persistence
            self._write_env_file(env_vars)
            results["env_file"] = self.env_file
            
            # Method 3: Save configuration to JSON for reference
            self._save_config(env_vars)
            
            # Check if Ollama is running and suggest restart
            ollama_running = self._check_ollama_running()
            if ollama_running:
                results["warning"] = "Ollama is currently running. Restart required for changes to take effect."
                results["restart_command"] = "pkill ollama && ollama serve"
                logger.warning(results["warning"])
            
            results["message"] = f"Ollama configured with {len(env_vars)} environment variables"
            logger.info(results["message"])
        
        except Exception as e:
            logger.error(f"Ollama configuration failed: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def _write_env_file(self, env_vars: Dict[str, str]):
        """Write environment variables to a shell file for persistence."""
        try:
            with open(self.env_file, 'w') as f:
                f.write("# Ollama Environment Configuration\n")
                f.write("# Generated by M1 Optimization Agent\n")
                f.write("# Source this file: source ~/.ollama_env\n\n")
                
                for key, value in env_vars.items():
                    f.write(f"export {key}={value}\n")
            
            logger.info(f"Environment variables written to {self.env_file}")
        except Exception as e:
            logger.error(f"Failed to write env file: {e}")
    
    def _save_config(self, config: Dict[str, str]):
        """Save configuration to JSON file."""
        try:
            with open(self.config_file, 'w') as f:
                json.dump(config, f, indent=2)
            logger.info(f"Configuration saved to {self.config_file}")
        except Exception as e:
            logger.error(f"Failed to save config: {e}")
    
    def _check_ollama_running(self) -> bool:
        """Check if Ollama is currently running."""
        result = self._run_command(['pgrep', '-x', 'ollama'])
        return result["success"] and result["stdout"].strip() != ""
    
    def get_current_config(self) -> Dict:
        """Get current Ollama configuration."""
        logger.info("Retrieving current Ollama configuration")
        
        results = {
            "success": True,
            "environment": {},
            "saved_config": {}
        }
        
        try:
            # Read from saved config file
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r') as f:
                    results["saved_config"] = json.load(f)
            
            # Try to read current environment variables
            env_vars = [
                "OLLAMA_NUM_PARALLEL",
                "OLLAMA_MAX_LOADED_MODELS",
                "OLLAMA_KEEP_ALIVE",
                "OLLAMA_NUM_CTX",
                "OLLAMA_HOST",
                "OLLAMA_MODELS"
            ]
            
            for var in env_vars:
                value = os.getenv(var)
                if value:
                    results["environment"][var] = value
            
            # Check Ollama status
            results["ollama_running"] = self._check_ollama_running()
            
            if self._check_ollama_running():
                # Try to get Ollama version
                version_result = self._run_command(['ollama', '--version'])
                if version_result["success"]:
                    results["ollama_version"] = version_result["stdout"].strip()
        
        except Exception as e:
            logger.error(f"Failed to get current config: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def restart_ollama(self) -> Dict:
        """Restart Ollama service to apply new configuration."""
        logger.info("Restarting Ollama service")
        
        results = {"success": True, "steps": []}
        
        try:
            # Stop Ollama
            if self._check_ollama_running():
                stop_result = self._run_command(['pkill', 'ollama'])
                if stop_result["success"]:
                    results["steps"].append("Stopped Ollama")
                    logger.info("Ollama stopped")
                    
                    # Wait a moment
                    import time
                    time.sleep(2)
            
            # Start Ollama in background
            # Note: This starts it in the background, but for production use,
            # users should use a proper service manager or launchd
            start_result = self._run_command(['nohup', 'ollama', 'serve', '&'])
            results["steps"].append("Started Ollama")
            logger.info("Ollama started")
            
            results["message"] = "Ollama restarted successfully"
        
        except Exception as e:
            logger.error(f"Failed to restart Ollama: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def create_modelfile(self, model_name: str, base_model: str,
                        context_size: int = 2048, temperature: float = 0.7,
                        system_prompt: Optional[str] = None) -> Dict:
        """Create a custom Modelfile with optimized parameters.
        
        Args:
            model_name: Name for the custom model
            base_model: Base model to use (e.g., 'llama3.2:3b')
            context_size: Context window size
            temperature: Temperature for generation
            system_prompt: Optional system prompt
        
        Returns:
            Dict with success status and modelfile path
        """
        logger.info(f"Creating Modelfile for {model_name}")
        
        results = {"success": True, "model_name": model_name}
        
        try:
            modelfile_path = f"config/Modelfile.{model_name}"
            
            with open(modelfile_path, 'w') as f:
                f.write(f"FROM {base_model}\n\n")
                
                # Set parameters
                f.write(f"PARAMETER num_ctx {context_size}\n")
                f.write(f"PARAMETER temperature {temperature}\n")
                f.write("PARAMETER num_thread 8\n")  # Optimize for M1
                
                # Add system prompt if provided
                if system_prompt:
                    f.write(f"\nSYSTEM {system_prompt}\n")
            
            results["modelfile_path"] = modelfile_path
            results["message"] = f"Modelfile created at {modelfile_path}"
            results["create_command"] = f"ollama create {model_name} -f {modelfile_path}"
            
            logger.info(results["message"])
        
        except Exception as e:
            logger.error(f"Failed to create Modelfile: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def optimize_for_ram(self, ram_gb: int) -> Dict:
        """Apply optimal Ollama configuration based on available RAM.
        
        Args:
            ram_gb: Available RAM in GB (8 or 16)
        
        Returns:
            Dict with applied configuration
        """
        logger.info(f"Optimizing Ollama for {ram_gb}GB RAM")
        
        if ram_gb == 8:
            # Conservative settings for 8GB
            return self.configure(
                num_parallel=1,
                max_loaded_models=1,
                keep_alive="5m",
                context_size=2048
            )
        elif ram_gb == 16:
            # More aggressive settings for 16GB
            return self.configure(
                num_parallel=2,
                max_loaded_models=1,
                keep_alive="10m",
                context_size=4096
            )
        else:
            # Default to conservative
            logger.warning(f"Unsupported RAM size: {ram_gb}GB. Using conservative settings.")
            return self.configure(
                num_parallel=1,
                max_loaded_models=1,
                keep_alive="5m"
            )
    
    def get_recommended_models(self, ram_gb: int) -> Dict:
        """Get recommended models based on available RAM.
        
        Args:
            ram_gb: Available RAM in GB
        
        Returns:
            Dict with recommended models and their configurations
        """
        recommendations = {
            8: [
                {
                    "model": "llama3.2:3b",
                    "quantization": "Q4_K_M",
                    "size_gb": 2.0,
                    "context": 2048,
                    "use_case": "General assistant, fast responses"
                },
                {
                    "model": "mistral:7b-instruct-q4_K_M",
                    "quantization": "Q4_K_M",
                    "size_gb": 4.1,
                    "context": 2048,
                    "use_case": "Reasoning and coding"
                },
                {
                    "model": "phi3:3.8b",
                    "quantization": "Q4_K_M",
                    "size_gb": 2.3,
                    "context": 4096,
                    "use_case": "Efficient general purpose"
                }
            ],
            16: [
                {
                    "model": "llama3.1:8b",
                    "quantization": "Q5_K_M",
                    "size_gb": 5.5,
                    "context": 4096,
                    "use_case": "High-quality general assistant"
                },
                {
                    "model": "mistral:7b-instruct-q8_0",
                    "quantization": "Q8_0",
                    "size_gb": 7.7,
                    "context": 4096,
                    "use_case": "Maximum precision reasoning"
                },
                {
                    "model": "codellama:13b-code-q4_K_M",
                    "quantization": "Q4_K_M",
                    "size_gb": 7.4,
                    "context": 8192,
                    "use_case": "Advanced coding tasks"
                }
            ]
        }
        
        return {
            "ram_gb": ram_gb,
            "recommendations": recommendations.get(ram_gb, recommendations[8]),
            "note": "Models listed are optimized for M1 architecture with appropriate quantization"
        }
