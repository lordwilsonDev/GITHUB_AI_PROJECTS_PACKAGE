#!/bin/bash

# M1 Optimization Agent - Startup Script
# This script applies optimizations and starts the agent
# Usage: ./startup.sh [--profile PROFILE_NAME] [--apply-optimizations] [agent_args...]

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Script directory
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

echo -e "${BLUE}=== M1 Optimization Agent Startup ===${NC}"
echo ""

# Parse arguments
PROFILE=""
APPLY_OPTIMIZATIONS=false
AGENT_ARGS=()

while [[ $# -gt 0 ]]; do
    case $1 in
        --profile)
            PROFILE="$2"
            shift 2
            ;;
        --apply-optimizations)
            APPLY_OPTIMIZATIONS=true
            shift
            ;;
        *)
            AGENT_ARGS+=("$1")
            shift
            ;;
    esac
done

# Check if running on Apple Silicon
CHIP=$(sysctl -n machdep.cpu.brand_string)
if [[ $CHIP == *"Apple"* ]]; then
    echo -e "${GREEN}✓${NC} Detected Apple Silicon: $CHIP"
else
    echo -e "${YELLOW}⚠${NC} Warning: Not running on Apple Silicon"
fi

# Check RAM
RAM_GB=$(sysctl -n hw.memsize | awk '{print int($1/1024/1024/1024)}')
echo -e "${GREEN}✓${NC} System RAM: ${RAM_GB}GB"

# Check if Ollama is installed
if command -v ollama &> /dev/null; then
    echo -e "${GREEN}✓${NC} Ollama is installed"
    OLLAMA_VERSION=$(ollama --version 2>&1 | head -n 1)
    echo "  Version: $OLLAMA_VERSION"
else
    echo -e "${YELLOW}⚠${NC} Ollama not found. Install from https://ollama.ai"
fi

# Check if Python 3 is available
if command -v python3 &> /dev/null; then
    PYTHON_VERSION=$(python3 --version)
    echo -e "${GREEN}✓${NC} $PYTHON_VERSION"
else
    echo -e "${RED}✗${NC} Python 3 not found"
    exit 1
fi

# Check if uv is installed
if command -v uv &> /dev/null; then
    echo -e "${GREEN}✓${NC} uv package manager is installed"
    UV_AVAILABLE=true
else
    echo -e "${YELLOW}⚠${NC} uv not found. Install with: curl -LsSf https://astral.sh/uv/install.sh | sh"
    UV_AVAILABLE=false
fi

echo ""

# Load and apply Ollama environment variables using launchctl setenv
if [ -f "$PROJECT_DIR/config/user_preferences.json" ]; then
    echo -e "${BLUE}Configuring Ollama environment variables...${NC}"
    
    # Extract Ollama settings from user preferences
    if command -v python3 &> /dev/null; then
        OLLAMA_NUM_PARALLEL=$(python3 -c "import json; f=open('$PROJECT_DIR/config/user_preferences.json'); d=json.load(f); print(d['ollama']['num_parallel'])" 2>/dev/null || echo "1")
        OLLAMA_MAX_LOADED=$(python3 -c "import json; f=open('$PROJECT_DIR/config/user_preferences.json'); d=json.load(f); print(d['ollama']['max_loaded_models'])" 2>/dev/null || echo "1")
        OLLAMA_KEEP_ALIVE=$(python3 -c "import json; f=open('$PROJECT_DIR/config/user_preferences.json'); d=json.load(f); print(d['ollama']['keep_alive'])" 2>/dev/null || echo "5m")
        OLLAMA_NUM_CTX=$(python3 -c "import json; f=open('$PROJECT_DIR/config/user_preferences.json'); d=json.load(f); print(d['ollama']['default_context_size'])" 2>/dev/null || echo "2048")
        
        # Set environment variables using launchctl setenv for user session persistence
        launchctl setenv OLLAMA_NUM_PARALLEL "$OLLAMA_NUM_PARALLEL"
        launchctl setenv OLLAMA_MAX_LOADED_MODELS "$OLLAMA_MAX_LOADED"
        launchctl setenv OLLAMA_KEEP_ALIVE "$OLLAMA_KEEP_ALIVE"
        launchctl setenv OLLAMA_NUM_CTX "$OLLAMA_NUM_CTX"
        
        # Also export for current session
        export OLLAMA_NUM_PARALLEL
        export OLLAMA_MAX_LOADED_MODELS
        export OLLAMA_KEEP_ALIVE
        export OLLAMA_NUM_CTX
        
        echo -e "${GREEN}✓${NC} Ollama environment configured:"
        echo "  OLLAMA_NUM_PARALLEL=$OLLAMA_NUM_PARALLEL"
        echo "  OLLAMA_MAX_LOADED_MODELS=$OLLAMA_MAX_LOADED"
        echo "  OLLAMA_KEEP_ALIVE=$OLLAMA_KEEP_ALIVE"
        echo "  OLLAMA_NUM_CTX=$OLLAMA_NUM_CTX"
        
        # Write to ~/.ollama_env for manual sourcing
        cat > ~/.ollama_env << EOF
# Ollama Environment Configuration
# Generated by M1 Optimization Agent
# Source this file: source ~/.ollama_env

export OLLAMA_NUM_PARALLEL=$OLLAMA_NUM_PARALLEL
export OLLAMA_MAX_LOADED_MODELS=$OLLAMA_MAX_LOADED
export OLLAMA_KEEP_ALIVE=$OLLAMA_KEEP_ALIVE
export OLLAMA_NUM_CTX=$OLLAMA_NUM_CTX
EOF
        echo -e "${GREEN}✓${NC} Environment saved to ~/.ollama_env"
    fi
fi

echo ""

# Apply optimizations if requested
if [ "$APPLY_OPTIMIZATIONS" = true ]; then
    echo -e "${BLUE}=== Applying System Optimizations ===${NC}"
    echo ""
    
    if [ -n "$PROFILE" ]; then
        echo -e "${YELLOW}Applying profile: $PROFILE${NC}"
        python3 "$PROJECT_DIR/scripts/apply_profile.py" --profile "$PROFILE"
    else
        echo -e "${YELLOW}Applying quick optimizations for ${RAM_GB}GB RAM${NC}"
        python3 "$PROJECT_DIR/scripts/quick_optimize.py" --ram "$RAM_GB" --level moderate
    fi
    
    echo ""
fi

# Check if virtual environment exists
if [ -d "$PROJECT_DIR/venv" ]; then
    echo -e "${GREEN}✓${NC} Activating virtual environment"
    source "$PROJECT_DIR/venv/bin/activate"
elif [ -d "$PROJECT_DIR/.venv" ]; then
    echo -e "${GREEN}✓${NC} Activating virtual environment"
    source "$PROJECT_DIR/.venv/bin/activate"
else
    echo -e "${YELLOW}⚠${NC} No virtual environment found"
    echo "  Create one with: cd $PROJECT_DIR && uv venv"
fi

echo ""
echo -e "${BLUE}=== Starting M1 Optimization Agent ===${NC}"
echo ""

# Change to project directory
cd "$PROJECT_DIR"

# Run the agent with any additional arguments
if [ ${#AGENT_ARGS[@]} -eq 0 ]; then
    # No arguments provided, show help
    python3 main_agent.py --help
else
    python3 main_agent.py "${AGENT_ARGS[@]}"
fi
